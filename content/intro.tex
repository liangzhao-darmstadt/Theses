\chapter{Introduction}
\label{cha:introduction}

% One of the most significant achievements in the 21st century is artificial intelligence (AI), especially machine learning (ML), which relies on using powerful computer hardware to accumulate and analyze massive data to improve the performance of algorithms. On the one hand, the customers enjoy the convenient services driven by ML algorithms. On the other hand, the users' sensitive data might be revealed to the third-party untrusted service provider. Even if the service providers are not malicious, other malicious entities such as internal employees, hackers, and national intelligence agencies could abuse the centralized database. Therefore, it is urgent to protect users' privacy while providing services. Since 2008, cryptographers have proposed many privacy-preserving ML algorithms based on secure multi-party computation (MPC). Secure multi-party computation enables multiple parties to perform computations with parties' inputs securely such that only the computation result is revealed. Secure multi-party computation is first proposed by Yao~\cite{yao1986generate} and becomes efficient for practical deployment until the late 2000s.

% Consider a typical scenario of privacy-preserving ML: Alice wishes to detect if she has the genetic disease but keep her genomic data secret. As a service provider, Bob has trained an ML model that can predict genetic disease when given genomic data. Similarly, Bob also wishes to keep the ML model parameters private to profit from it continuously. One solution is introducing a trusted third party to do genome sequencing with Alice's genomic data and Bob's ML model. However, a trusted third party barely exists in practice. Instead, Alice and Bob can deploy MPC protocol to solve the problem by simulating a trusted third party such that only the gene detection result is revealed.

% Although MPC can guarantee the users' computational privacy, an adversary can still infer users' sensitive information from the computation result by executing attacks such as membership inference. For example, several hospitals jointly trained an ML model using MPC protocols with their patient's data, and only the trained ML model parameters were revealed to them. An adversary doesn't have access to the patients' data but can still infer if a particular patient's data is involved in the ML training and further derive additional sensitive information. Dwork et al.~\cite{dwork2006differential} formalize this privacy loss by introducing the concept of differential privacy (DP). One approach to guarantee DP is to add calibrated noise to the revealed computation result~\cite{dwork2006differential, dwork2006calibrating}.

% Naturally, a better privacy-preserving method is to combine MPC and DP as prior works~\cite{eigner2014differentially,byrd2020differentially,pettai2015combining}. However, as far as we know, no prior works have considered the security issues under practical implementations. As Mironov~\cite{mironov2012significance} shows, the textbook noise generation methods can breach differential privacy under floating-point arithmetic. Our work attempts to fill the vacuum by providing efficient and secure MPC-DP protocols and implementations.

% \textbf{Research Goal}
% In this thesis, we study how to achieve DP under MPC setting securely, i.e., constructing MPC protocols for secure noise generation and output perturbation. In addition, we aim at efficient MPC protocols by evaluating various optimization techniques and their practical performance.

% \textbf{Contributions}
% We invest the secure noise generation methods and prove that these noise under floating-point number representation can satisfy the differential privacy. A major part of this thesis deals with the construction and optimizations of MPC protocols for noise generation as MPC is still significantly slower than plaintext computations. Specifically, we consider the outsourcing scenario of MPC, i.e., users first secret share their private input to multiple ($N \geq 2$) non-colluding computing parties and the computing parties execute the MPC protocols for desired function computation and noise addition. We use oblivious transfer (OT) extensively for the multi-party computation to improve efficiency for operations such as bit-vector multiplication, oblivious random access, and arithmetic comparison. In addition, we use HyCC~\cite{buscher2018hycc}, i.e., an automated compilation tool for generating circuits for hybrid MPC protocols.

% \textbf{Thesis Outline}
% The remainder of this thesis is structured in five parts.

% In Part 1 - Preliminaries~\autoref{cha:prelim}, we discuss basic notations and theoretical background of MPC and DP. In chapter 2, we recap the background and overview of the multiparty computation. In chapter 3, we review the differential privacy theory and describe an example application for intuition.

% In Part 2 - Secure Differentially Private Mechanisms~\autoref{cha:secureDPMechanisms}, we first describe the attack when using textbook noise generation under floating-point arithmetic. Then we introduce several existing secure noise generations and differentially private mechanisms, i.e., snapping mechanism~\autoref{sec:snappingMechanism}, Integer-Scaling mechanisms~\autoref{sec:integerScalingMechanism} and discrete Gaussian mechanism~\autoref{sec:discreteGaussianmechanism}.

% In Part 3 - General Procedure for MPC-DP Protocols~\autoref{cha:ProcedureMPCDP}, we first restate the investigated research problem and describe the general procedure for realizing differentially private mechanisms under MPC. Then we review the prior works for combining MPC and DP.

% In Part 4 - Secure MPC-DP Protocols~\autoref{cha:MPCProtocolsforDifferentiallyPrivateMechanisms}, we first describe the building blocks for our MPC protocols in~\autoref{sec:MPCBuildingBlocks}. Then, we provide the MPC protocols for differentially private mechanisms (cf.~\autoref{cha:secureDPMechanisms}).

% In Part 5 - Implementation and Evaluations~\autoref{cha:evaluation}, we implement and evaluate our MPC protocols (cf.~\autoref{cha:MPCProtocolsforDifferentiallyPrivateMechanisms}).


% \ai has ushered in rapid development since 2012, where the number of software projects that rely on \ai has increased significantly~\cite{clark_2015}. As an essential branch of \ai, \machinelearning heavily relies on massive data analysis, posing severe privacy concerns as the individual's sensitive information in the highly centralized database may be misused. 

Technologies such as \machinelearning rely heavily on massive data analysis and pose severe privacy concerns as the individual's information in the highly centralized database may be misused. Privacy violation comes in many forms and is not directly visible. Therefore, it is crucial to provide appropriate privacy protections for user data.
Many methods have been explored before to protect the privacy of individuals in the database.
Since 2008, cryptographers have proposed \ppml algorithms to avoid the privacy breach of the training dataset based on \smpc. \smpc enables multiple parties to securely perform distributed computations with parties' private inputs so that only the computation results are revealed.

Let us consider a typical scenario of \ppml: Alice wishes to investigate if she has genetic disorders while keeping her genomic data secret. As a service provider, Bob has trained an \machinelearning model that can predict genetic disorders given genomic data. However, Bob wants to keep his \machinelearning model private as it is his intellectual property that he aims to monetize. One unrealistic solution would be to rely on a trusted third party to analyze Alice's genetic data with Bob's \machinelearning model. However, as a trusted third party does rarely exist in practice, Alice and Bob can deploy a \smpc protocol to simulate a trusted third party.

Although \smpc can guarantee the users' computational privacy, an adversary can still infer users' sensitive information from the computation output. Shokri et al.~\cite{shokri2017membership} showed a membership inference attack that can determine if a data record was in the model's training dataset by making an adversarial usage of \machinelearning algorithms. One solution to mitigate such an attack is to deploy differentially private mechanisms. The concept of \differentialprivacy was introduced by Dwork et al.~\cite{dwork2006differential, dwork2006calibrating} that limits private information disclosure by adding calibrated noise to the revealed output.
However, Mironov~\cite{mironov2012significance} and Jin et al.~\cite{jin2022we} demonstrated a series of attacks against the differentially private mechanisms implemented under floating-point arithmetics.

To the best of our knowledge, most prior works~\cite{rastogi2010differentially,Elaine2011Privacy,acs2011have,chan2012privacy,eigner2014differentially,wu2016inherit,bindschaedler2017achieving,jayaraman2018distributed,truex2019hybrid,knott2021crypten,yuan2021practical} that combine \smpc and \differentialprivacy do not consider the above security issues.
% However, to the best of our knowledge, none of them have considered the security issues of \differentialprivacy that arise in many practical implementations. 
This work attempts to fill this gap by providing \textit{secure} noise generation methods in multi-party settings based on the state-of-the-art \smpc framework MOTION~\cite{braun2022motion}.
The start point of this work is the secure noise generation methods and differentially private mechanisms from works~\cite{mironov2012significance,googleDP2019,canonne2020discrete}.
The fundamental idea of secure noise generation is to generate discrete noise and re-scale it precisely under floating-point implementation to simulate continuous noise such that the noise satisfies the differential privacy requirements.
We investigate the potential of applying these noise generation methods in the \smpc.
Besides, we aim to achieve \differentialprivacy and maintain the optimal utility of the computation result by adding a minimal amount of noise.
% We also aim at efficient MPC protocols by evaluating the performance of various \smpc optimization techniques~\cite{braun2022motion}.

\paragraph{Contributions.}
\label{para:Contributions}
\begin{enumerate}

    \item The noise is generated in a fully distributed manner that maintains the optimal utility of the aggregate statistics by introducing the minimal amount of noise required to satisfy \differentialprivacy.
          We consider the outsourcing scenario~\cite{kamara2011secure}, i.e., the data owners first secret share their private inputs to multiple ($N \geq 2$) non-colluding computation parties, and the computation parties execute the \smpc protocols to compute the desired functionality and perturb the result. MOTION~\cite{braun2022motion} supports full-threshold security, and the computation result is secure if at least one computation party is honest and non-collusive. Therefore, the computation parties can jointly generate the shares of a publicly unknown noise with the same magnitude as the noise generated by a single trusted server.

    \item We support a variety of differentially private mechanisms such as the (discrete) Laplace mechanism~\cite{chan2012privacy,ghosh2012universally,dwork2014algorithmic}, the (discrete) Gaussian mechanism~\cite{dwork2014algorithmic, canonne2020discrete}, and the snapping mechanism~\cite{mironov2012significance} in \smpc.

    \item We implement fixed-point and floating-point operations in the binary circuit-based and the arithmetic sharing \smpc protocols and evaluate the performance. We use \simd instructions to eliminate the independent iterations in the sampling algorithms and improve the protocol performance.

\end{enumerate}
% This thesis first 

% \TODO{describe novel MPC techniques...}

\textbf{Thesis Outline.}
% \TODO{add after revision}
This thesis is organized as follows:
Chapter 2 gives the preliminaries on the concept of secure multiparty computation and differential privacy with motivating examples and formal definitions.
Chapter 3 presents a summary and discussion of the related works.
Chapter 4 describes the details of the secure noise generation methods, differentially private mechanisms and our modifications.
Chapter 5 provides the procedure to combine \smpc protocols and differentially private mechanisms, building blocks, and the \smpc protocols for differentially private mechanisms.
Chapter 6 evaluates the performance of fixed-point and floating-point and the differentially private mechanisms.
Chapter 7 concludes this work by summarizing the results and pointing out several directions for further research.
% \TODO{add after revision}
% The remainder of this thesis is structured in three parts.
% In Part 1 - Preliminaries (ref), we discuss basic notations and theoretical background of MPC and DP. In chapter 2 we recap the background and overview of the multiparty computation. In chapter 3, we review the differential privacy theory and describe an example application for intuition. 

% In Part 2 - Secure Noise Generation, we first describe the attack when using textbook noise generation under floating-point implementation. Then we introduce several secure noise generations under floating-point representation.

% In Part 3 - MPC-DP Protocols, we first explain the advantages and difficulties of combining MPC and DP, and then propose the general MPC-DP framework in chapter x. In chapter x and x, we provide the construction and optimization of MPC protocols for noise generation. In chapter x, we implement the MPC protocols and evaluate the MPC-DP framework with typical machine learning problems. 
