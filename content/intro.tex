\chapter{Introduction}
\label{cha:introduction}

% One of the most significant achievements in the 21st century is artificial intelligence (AI), especially machine learning (ML), which relies on using powerful computer hardware to accumulate and analyze massive data to improve the performance of algorithms. On the one hand, the customers enjoy the convenient services driven by ML algorithms. On the other hand, the users' sensitive data might be revealed to the third-party untrusted service provider. Even if the service providers are not malicious, other malicious entities such as internal employees, hackers, and national intelligence agencies could abuse the centralized database. Therefore, it is urgent to protect users' privacy while providing services. Since 2008, cryptographers have proposed many privacy-preserving ML algorithms based on secure multi-party computation (MPC). Secure multi-party computation enables multiple parties to perform computations with parties' inputs securely such that only the computation result is revealed. Secure multi-party computation is first proposed by Yao~\cite{yao1986generate} and becomes efficient for practical deployment until the late 2000s.

% Consider a typical scenario of privacy-preserving ML: Alice wishes to detect if she has the genetic disease but keep her genomic data secret. As a service provider, Bob has trained an ML model that can predict genetic disease when given genomic data. Similarly, Bob also wishes to keep the ML model parameters private to profit from it continuously. One solution is introducing a trusted third party to do genome sequencing with Alice's genomic data and Bob's ML model. However, a trusted third party barely exists in practice. Instead, Alice and Bob can deploy MPC protocol to solve the problem by simulating a trusted third party such that only the gene detection result is revealed.

% Although MPC can guarantee the users' computational privacy, an adversary can still infer users' sensitive information from the computation result by executing attacks such as membership inference. For example, several hospitals jointly trained an ML model using MPC protocols with their patient's data, and only the trained ML model parameters were revealed to them. An adversary doesn't have access to the patients' data but can still infer if a particular patient's data is involved in the ML training and further derive additional sensitive information. Dwork et al.~\cite{dwork2006differential} formalize this privacy loss by introducing the concept of differential privacy (DP). One approach to guarantee DP is to add calibrated noise to the revealed computation result~\cite{dwork2006differential, dwork2006calibrating}.

% Naturally, a better privacy-preserving method is to combine MPC and DP as prior works~\cite{eigner2014differentially,byrd2020differentially,pettai2015combining}. However, as far as we know, no prior works have considered the security issues under practical implementations. As Mironov~\cite{mironov2012significance} shows, the textbook noise generation methods can breach differential privacy under floating-point arithmetic. Our work attempts to fill the vacuum by providing efficient and secure MPC-DP protocols and implementations.

% \textbf{Research Goal}
% In this thesis, we study how to achieve DP under MPC setting securely, i.e., constructing MPC protocols for secure noise generation and output perturbation. In addition, we aim at efficient MPC protocols by evaluating various optimization techniques and their practical performance.

% \textbf{Contributions}
% We invest the secure noise generation methods and prove that these noise under floating-point number representation can satisfy the differential privacy. A major part of this thesis deals with the construction and optimizations of MPC protocols for noise generation as MPC is still significantly slower than plaintext computations. Specifically, we consider the outsourcing scenario of MPC, i.e., users first secret share their private input to multiple ($N \geq 2$) non-colluding computing parties and the computing parties execute the MPC protocols for desired function computation and noise addition. We use oblivious transfer (OT) extensively for the multi-party computation to improve efficiency for operations such as bit-vector multiplication, oblivious random access, and arithmetic comparison. In addition, we use HyCC~\cite{buscher2018hycc}, i.e., an automated compilation tool for generating circuits for hybrid MPC protocols.

% \textbf{Thesis Outline}
% The remainder of this thesis is structured in five parts.

% In Part 1 - Preliminaries~\autoref{cha:prelim}, we discuss basic notations and theoretical background of MPC and DP. In chapter 2, we recap the background and overview of the multiparty computation. In chapter 3, we review the differential privacy theory and describe an example application for intuition.

% In Part 2 - Secure Differentially Private Mechanisms~\autoref{cha:secureDPMechanisms}, we first describe the attack when using textbook noise generation under floating-point arithmetic. Then we introduce several existing secure noise generations and differentially private mechanisms, i.e., snapping mechanism~\autoref{sec:snappingMechanism}, Integer-Scaling mechanisms~\autoref{sec:integerScalingMechanism} and discrete Gaussian mechanism~\autoref{sec:discreteGaussianmechanism}.

% In Part 3 - General Procedure for MPC-DP Protocols~\autoref{cha:ProcedureMPCDP}, we first restate the investigated research problem and describe the general procedure for realizing differentially private mechanisms under MPC. Then we review the prior works for combining MPC and DP.

% In Part 4 - Secure MPC-DP Protocols~\autoref{cha:MPCProtocolsforDifferentiallyPrivateMechanisms}, we first describe the building blocks for our MPC protocols in~\autoref{sec:MPCBuildingBlocks}. Then, we provide the MPC protocols for differentially private mechanisms (cf.~\autoref{cha:secureDPMechanisms}).

% In Part 5 - Implementation and Evaluations~\autoref{cha:evaluation}, we implement and evaluate our MPC protocols (cf.~\autoref{cha:MPCProtocolsforDifferentiallyPrivateMechanisms}).


% \ai has ushered in rapid development since 2012, where the number of software projects that rely on \ai has increased significantly~\cite{clark_2015}. As an essential branch of \ai, \machinelearning heavily relies on massive data analysis, posing severe privacy concerns as the individual's sensitive information in the highly centralized database may be misused. 

Technologies such as \machinelearning rely heavily on massive data analysis, posing severe privacy concerns as the individual's information in the highly centralized database may be misused. Privacy violation comes in many forms and is not directly visible. Therefore, it is crucial to provide appropriate privacy protections for the users' data.

Many methods have been tried before to protect the privacy of individuals in the database.
Since 2008, cryptographers have proposed many \ppml algorithms that are based on \smpc. Secure Multi-Party Computation enables multiple parties to perform distributed computations with parties' inputs securely such that only the computation result is revealed.

Let us consider a typical scenario of \ppml: Alice wishes to investigate if she has the genetic disorders while keeping her genomic data secret. As a service provider, Bob has trained a \machinelearning model that can predict genetic disorders given genomic data. Besides, Bob wants to keep his \machinelearning model private as his intellectual property. One unrealistic solution would be to rely on a trusted third party to analyze Alice's genetic data with Bob's \machinelearning model. However, since such a trusted third party rarely exists in practice, Alice and Bob can deploy an \smpc protocol to simulate a trusted third party.

Although \smpc can guarantee the users' computational privacy, an adversary can still infer users' sensitive information from the computation output. Shokri et al.~\cite{shokri2017membership} showed a membership inference attack that can determine if a data record was in the model's training dataset by making an adversarial usage of \machinelearning algorithms. One solution to resist such an attack is to deploy differentially private algorithms. The concept of \differentialprivacy was introduced by Dwork et.al~\cite{dwork2006differential, dwork2006calibrating} that limits private information disclosure by adding calibrated noise to the revealed computation output.

To achieve both computational and output privacy, the natural approach is to combine both \smpc and \differentialprivacy as already investigated by prior works~\cite{eigner2014differentially, pettai2015combining, truex2019hybrid,byrd2020differentially}. However, to the best of our knowledge, none of them have considered the security issues of \differentialprivacy that arise in many practical implementations. Mironov~\cite{mironov2012significance} showed that the textbook Laplace noise generation methods could break \differentialprivacy guarantee due to the finite precision and rounding effects of floating-point arithmetic. Our work attempts to fill this gap by providing \textit{secure} noise generation methods and satisfy \differentialprivacy guarantee in \smpc setting based on the state-of-the-art \smpc framework MOTION~\cite{braun2022motion}.

% \textbf{Research Goal.}
% In this thesis, we investigate how to combine \differentialprivacy and \smpc under in a secure manner. 
% Specifically, we design novel \smpc protocols for differentially private algorithms and secure noise generation methods based on prior works~\cite{mironov2012significance,googleDP2019,canonne2020discrete}. 
Prior works~\cite{mironov2012significance,googleDP2019,canonne2020discrete} proposed secure noise generation methods and differentially private mechanisms that are proved to satisfy \differentialprivacy guarantee.
This work evaluate their potential in the \smpc setting.
The basic idea of secure noise generation is to generate discrete noise and re-scale it precisely under floating-point implementation such that the distribution of the noise \textit{really} satisfy differential privacy requirements. Besides, we aim to achieving \differentialprivacy and maintain the utility of the computation result by adding the minimal amount of noise. We also aim at efficient MPC protocols by evaluating the performance of various \smpc optimization techniques~\cite{braun2022motion}.

\textbf{Contributions.}
We support a variety of differentilly private mechanisms such as (discrete) Laplace mechanism~\cite{chan2012privacy,ghosh2012universally,dwork2014algorithmic}, (discrete) Gaussian mechanism~\cite{dwork2014algorithmic, canonne2020discrete} and snapping mechanism~\cite{mironov2012significance} that are suitable for various applications such as web analytics and health services.
We consider the outsourcing scenario~\cite{kamara2011secure}, i.e., the data owners first secret share their private input to multiple ($N \geq 2$) non-colluding computation parties, and the computation parties execute the \smpc protocols to securely compute the desired functionality and perturb the result. We rely on the MOTION framework~\cite{braun2022motion} that supports full-threshold security, which means that the computation result is secure as long as one computation party is honest. Therefore, the computation parties can jointly generate the shares of a publicly unknown noise with the same magnitude as the noise generated by a single trusted server.
This guarantees that the computation result is perturbed with minimal amount of noise required to achieve DP.
To find the most efficient implementation of the arithmetic operations in MPC, we explore both fixed-point and floating-point arithmetic and implement them in the binary circuit-based and arithmetic sharing approaches.
In constructing MPC protocols, we use Single Instruction Multiple Data (SIMD) instructions to eliminate the independent iterations in the sampling algorithms and improve the performance.
% This thesis first 

% \TODO{describe novel MPC techniques...}

\textbf{Thesis Outline.}
% \TODO{add after revision}
This thesis is organized as follows:
Chapter 2 gives the preliminaries on the concept of secure multiparty computation and differential privacy with motivating examples and formal definitions.
Chapter 3 describes the details of the differentially private mechanisms, secure noise generation methods, and our modifications.
Chapter 4 provides the procedure to combine \smpc protocols and differentially private mechanisms, \smpc building blocks, and the \smpc protocols for differentially private mechanisms.
Chapter 5 evaluates the performance of our \smpc protocols.
\TODO{add after revision}
% The remainder of this thesis is structured in three parts.
% In Part 1 - Preliminaries (ref), we discuss basic notations and theoretical background of MPC and DP. In chapter 2 we recap the background and overview of the multiparty computation. In chapter 3, we review the differential privacy theory and describe an example application for intuition. 

% In Part 2 - Secure Noise Generation, we first describe the attack when using textbook noise generation under floating-point implementation. Then we introduce several secure noise generations under floating-point representation.

% In Part 3 - MPC-DP Protocols, we first explain the advantages and difficulties of combining MPC and DP, and then propose the general MPC-DP framework in chapter x. In chapter x and x, we provide the construction and optimization of MPC protocols for noise generation. In chapter x, we implement the MPC protocols and evaluate the MPC-DP framework with typical machine learning problems. 
